from dotenv import load_dotenv
load_dotenv()

import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# ====== ã‚¢ãƒ—ãƒªåŸºæœ¬è¨­å®š ======
st.set_page_config(
    page_title="LangChain Ã— Streamlit ãƒ‡ãƒ¢",
    page_icon="ğŸ“š",
    layout="centered"
)

st.title("ğŸ“š LangChain Ã— Streamlit ãƒ‡ãƒ¢")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€å…¥åŠ›ã—ãŸè³ªå•ã«å¯¾ã—ã¦ã€é¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã§å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

1. è³ªå•ã‚’å…¥åŠ›  
2. å°‚é–€å®¶ã‚’é¸æŠ  
3. ã€Œé€ä¿¡ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
""")

# ====== å°‚é–€å®¶é¸æŠ ======
expert_choice = st.radio(
    "å°‚é–€å®¶ã‚’é¸æŠã—ã¦ãã ã•ã„",
    ("å¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼", "æ–™ç†ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ")
)

# ====== ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ› ======
user_input = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

# ====== LLMå‘¼ã³å‡ºã—é–¢æ•° ======
def get_llm_response(user_question: str, expert: str) -> str:
    """å…¥åŠ›ã¨å°‚é–€å®¶é¸æŠã‚’ã‚‚ã¨ã«LLMã‹ã‚‰å›ç­”ã‚’å–å¾—"""
    if expert == "å¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼":
        system_prompt = "ã‚ãªãŸã¯å¥åº·ã«é–¢ã™ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚å®‰å…¨ã§å®Ÿç”¨çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
    else:
        system_prompt = "ã‚ãªãŸã¯æ–™ç†ã«é–¢ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ãƒ¬ã‚·ãƒ”ã‚„èª¿ç†æ³•ã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"

    # ChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    client = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

    # invoke() ã§å®Ÿè¡Œ
    response = client.invoke([
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_question}
    ])

    return response.content  # â†ã“ã“ã§textã‚’è¿”ã™

# ====== é€ä¿¡ãƒœã‚¿ãƒ³ ======
if st.button("é€ä¿¡"):
    if not user_input.strip():
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­â€¦"):
            answer = get_llm_response(user_input, expert_choice)
        st.success("ğŸ’¬ å›ç­”:")
        st.write(answer)